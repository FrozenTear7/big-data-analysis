{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADZD Lab 6 - MLlib\n",
    "\n",
    "## Autor: Paweł Mendroch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLlib Exercises\n",
    "\n",
    "\n",
    "http://spark.apache.org/docs/latest/ml-statistics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://2bafd12e9dbe:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcd2008dd90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "(-1.80e+0,  5.60e+0, 0  ),\n",
    "( 1.20e+0, -1.30e+0, 1  ),\n",
    "( 2.40e-1,  9.20e-1, -1 ),\n",
    "( 1.50e-1,  5.50e+0, 2  ),\n",
    "( 1.80e+0,  3.00e-1, 1  ),\n",
    "(-1.80e+0,  3.10e+0, 0  ),\n",
    "( 2.20e+0,  8.70e+0, 0  ),\n",
    "(-8.70e-1, -1.60e+0, 1  ),\n",
    "(-1.50e+0,  2.10e+0, -2 ),\n",
    "(-9.90e-1, -5.10e+0, 2  ),\n",
    "( 2.00e+0,  3.10e-2, 0  ),\n",
    "(-9.50e-1,  2.60e+0, 0  ),\n",
    "(-1.80e+0, -7.80e-2, 0  ),\n",
    "(-6.30e-1,  1.70e+0, 1  ),\n",
    "( 2.40e-1,  7.10e-1, 0  ),\n",
    "(-2.50e-1, -2.20e+0, 0  ),\n",
    "( 1.70e+0,  6.60e-1, 2  ),\n",
    "(-2.50e+0,  6.80e-1, -1 ),\n",
    "(-1.10e+0,  3.50e+0, -2 ),\n",
    "(-1.30e+0,  3.40e+0, 0  ),\n",
    "( 3.60e-1,  6.50e+0, 0  ),\n",
    "(-1.80e+0,  6.70e+0, -1 ),\n",
    "( 8.40e-1, -6.60e-1, 2  ),\n",
    "(-1.60e-1,  1.80e+0, 1  ),\n",
    "(-2.80e+0,  1.60e+0, 0  ),\n",
    "(-8.70e-2,  4.70e-1, 1  ),\n",
    "( 7.50e-1,  6.10e+0, 0  ),\n",
    "( 8.90e-1,  5.40e+0, -2 ),\n",
    "(-4.30e-1,  5.20e+0, -1 ),\n",
    "(-7.00e-1 ,  5.80e+0 , 0)\n",
    "], [\"x1\", \"x2\", \"x3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.A.\n",
    "**TODO:** Calculate descriptive statistics (see https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/DataFrame.html#describe(scala.collection.Seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+-------------------+\n",
      "|summary|                 x1|                x2|                 x3|\n",
      "+-------+-------------------+------------------+-------------------+\n",
      "|  count|                 30|                30|                 30|\n",
      "|   mean|-0.3032333333333333|2.2710999999999997|0.13333333333333333|\n",
      "| stddev| 1.3388872807576868|3.1520939329387003| 1.1366415543118709|\n",
      "|    min|               -2.8|              -5.1|                 -2|\n",
      "|    max|                2.2|               8.7|                  2|\n",
      "+-------+-------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(\"x1\", \"x2\", \"x3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.B.\n",
    "\n",
    "**TODO:** Check which features have normal distribution (see http://spark.apache.org/docs/latest/api/java/org/apache/spark/ml/stat/KolmogorovSmirnovTest.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'pValue'>\n",
      "Column<b'statistic'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import KolmogorovSmirnovTest\n",
    "\n",
    "ks_test = KolmogorovSmirnovTest()\n",
    "\n",
    "\n",
    "for x in ks_test.test(df, \"x1\", \"norm\"):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc: http://spark.apache.org/docs/latest/ml-datasource.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from file data/mllib/sample_libsvm_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(780,[127,128,129...|\n",
      "|  1.0|(780,[158,159,160...|\n",
      "|  1.0|(780,[124,125,126...|\n",
      "|  1.0|(780,[152,153,154...|\n",
      "|  1.0|(780,[151,152,153...|\n",
      "|  0.0|(780,[129,130,131...|\n",
      "|  1.0|(780,[158,159,160...|\n",
      "|  1.0|(780,[99,100,101,...|\n",
      "|  0.0|(780,[154,155,156...|\n",
      "|  0.0|(780,[127,128,129...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, features=SparseVector(780, {127: 51.0, 128: 159.0, 129: 253.0, 130: 159.0, 131: 50.0, 154: 48.0, 155: 238.0, 156: 252.0, 157: 252.0, 158: 252.0, 159: 237.0, 181: 54.0, 182: 227.0, 183: 253.0, 184: 252.0, 185: 239.0, 186: 233.0, 187: 252.0, 188: 57.0, 189: 6.0, 207: 10.0, 208: 60.0, 209: 224.0, 210: 252.0, 211: 253.0, 212: 252.0, 213: 202.0, 214: 84.0, 215: 252.0, 216: 253.0, 217: 122.0, 235: 163.0, 236: 252.0, 237: 252.0, 238: 252.0, 239: 253.0, 240: 252.0, 241: 252.0, 242: 96.0, 243: 189.0, 244: 253.0, 245: 167.0, 262: 51.0, 263: 238.0, 264: 253.0, 265: 253.0, 266: 190.0, 267: 114.0, 268: 253.0, 269: 228.0, 270: 47.0, 271: 79.0, 272: 255.0, 273: 168.0, 289: 48.0, 290: 238.0, 291: 252.0, 292: 252.0, 293: 179.0, 294: 12.0, 295: 75.0, 296: 121.0, 297: 21.0, 300: 253.0, 301: 243.0, 302: 50.0, 316: 38.0, 317: 165.0, 318: 253.0, 319: 233.0, 320: 208.0, 321: 84.0, 328: 253.0, 329: 252.0, 330: 165.0, 343: 7.0, 344: 178.0, 345: 252.0, 346: 240.0, 347: 71.0, 348: 19.0, 349: 28.0, 356: 253.0, 357: 252.0, 358: 195.0, 371: 57.0, 372: 252.0, 373: 252.0, 374: 63.0, 384: 253.0, 385: 252.0, 386: 195.0, 399: 198.0, 400: 253.0, 401: 190.0, 412: 255.0, 413: 253.0, 414: 196.0, 426: 76.0, 427: 246.0, 428: 252.0, 429: 112.0, 440: 253.0, 441: 252.0, 442: 148.0, 454: 85.0, 455: 252.0, 456: 230.0, 457: 25.0, 466: 7.0, 467: 135.0, 468: 253.0, 469: 186.0, 470: 12.0, 482: 85.0, 483: 252.0, 484: 223.0, 493: 7.0, 494: 131.0, 495: 252.0, 496: 225.0, 497: 71.0, 510: 85.0, 511: 252.0, 512: 145.0, 520: 48.0, 521: 165.0, 522: 252.0, 523: 173.0, 538: 86.0, 539: 253.0, 540: 225.0, 547: 114.0, 548: 238.0, 549: 253.0, 550: 162.0, 566: 85.0, 567: 252.0, 568: 249.0, 569: 146.0, 570: 48.0, 571: 29.0, 572: 85.0, 573: 178.0, 574: 225.0, 575: 253.0, 576: 223.0, 577: 167.0, 578: 56.0, 594: 85.0, 595: 252.0, 596: 252.0, 597: 252.0, 598: 229.0, 599: 215.0, 600: 252.0, 601: 252.0, 602: 252.0, 603: 196.0, 604: 130.0, 622: 28.0, 623: 199.0, 624: 252.0, 625: 252.0, 626: 253.0, 627: 252.0, 628: 252.0, 629: 233.0, 630: 145.0, 651: 25.0, 652: 128.0, 653: 252.0, 654: 253.0, 655: 252.0, 656: 141.0, 657: 37.0}))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"file:///usr/local/spark/data/mllib/sample_libsvm_data.txt\"\n",
    "\n",
    "df = spark.read.format(\"libsvm\").option(\"numFeatures\", \"780\").load(file)\n",
    "df.show(10)\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.A\n",
    "**TODO:** Load wine data from https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/wine.scale\n",
    "Dataset description: http://archive.ics.uci.edu/ml/datasets/Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features=SparseVector(13, {0: 0.6842, 1: -0.6166, 2: 0.1444, 3: -0.4845, 4: 0.2391, 5: 0.2552, 6: 0.1477, 7: -0.434, 8: 0.1861, 9: -0.256, 10: -0.0894, 11: 0.9414, 12: 0.1227}))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"wine.scale\"\n",
    "\n",
    "winedf = spark.read.format(\"libsvm\").load(file)\n",
    "winedf.show(10)\n",
    "winedf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn a decision tree to classify wines (see http://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier). Split data into training and testing sets: winedf.randomSplit([0.7, 0.3]).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       1.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|       1.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|       1.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|       1.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|       1.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|       1.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|       1.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|       1.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|       1.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|       1.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Test Error = 0.112903 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = winedf.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "model = dt.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(10)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.A\n",
    "**TODO:** Add VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4) to the pipeline and compare the accuracy and the trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       1.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|       1.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|       1.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|       1.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|       2.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|       1.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|       1.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|       1.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|       1.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "|       1.0|  1.0|(13,[0,1,2,3,4,5,...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Test Error = 0.0638298 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = winedf.randomSplit([0.7, 0.3])\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(winedf)\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
    "\n",
    "# model = dt.fit(trainingData)\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(10)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load csv wine dataset from https://gist.github.com/tijptjik/9408623#file-wine-csv\n",
    "\n",
    "**Note!** Dots (.) should be removed in the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----------+----+----+---+-------+----------+--------------------+-------+---------+----+----+-------+\n",
      "|label|Alcohol|Malic acid| Ash| Acl| Mg|Phenols|Flavanoids|Nonflavanoid phenols|Proanth|Color int| Hue|  OD|Proline|\n",
      "+-----+-------+----------+----+----+---+-------+----------+--------------------+-------+---------+----+----+-------+\n",
      "|    1|  14.23|      1.71|2.43|15.6|127|    2.8|      3.06|                0.28|   2.29|     5.64|1.04|3.92|   1065|\n",
      "|    1|   13.2|      1.78|2.14|11.2|100|   2.65|      2.76|                0.26|   1.28|     4.38|1.05| 3.4|   1050|\n",
      "|    1|  13.16|      2.36|2.67|18.6|101|    2.8|      3.24|                 0.3|   2.81|     5.68|1.03|3.17|   1185|\n",
      "|    1|  14.37|      1.95| 2.5|16.8|113|   3.85|      3.49|                0.24|   2.18|      7.8|0.86|3.45|   1480|\n",
      "|    1|  13.24|      2.59|2.87|21.0|118|    2.8|      2.69|                0.39|   1.82|     4.32|1.04|2.93|    735|\n",
      "|    1|   14.2|      1.76|2.45|15.2|112|   3.27|      3.39|                0.34|   1.97|     6.75|1.05|2.85|   1450|\n",
      "|    1|  14.39|      1.87|2.45|14.6| 96|    2.5|      2.52|                 0.3|   1.98|     5.25|1.02|3.58|   1290|\n",
      "|    1|  14.06|      2.15|2.61|17.6|121|    2.6|      2.51|                0.31|   1.25|     5.05|1.06|3.58|   1295|\n",
      "|    1|  14.83|      1.64|2.17|14.0| 97|    2.8|      2.98|                0.29|   1.98|      5.2|1.08|2.85|   1045|\n",
      "|    1|  13.86|      1.35|2.27|16.0| 98|   2.98|      3.15|                0.22|   1.85|     7.22|1.01|3.55|   1045|\n",
      "+-----+-------+----------+----+----+---+-------+----------+--------------------+-------+---------+----+----+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "[('label', 'int'), ('Alcohol', 'double'), ('Malic acid', 'double'), ('Ash', 'double'), ('Acl', 'double'), ('Mg', 'int'), ('Phenols', 'double'), ('Flavanoids', 'double'), ('Nonflavanoid phenols', 'double'), ('Proanth', 'double'), ('Color int', 'double'), ('Hue', 'double'), ('OD', 'double'), ('Proline', 'int')]\n"
     ]
    }
   ],
   "source": [
    "file = \"wine.csv\"\n",
    "winedf2 = spark.read.format(\"csv\")\\\n",
    "    .options(inferSchema=\"true\", header=\"true\")\\\n",
    "    .load(file)\\\n",
    "    .withColumnRenamed(\"Wine\", \"label\")\\\n",
    "    .withColumnRenamed(\"Malic.acid\", \"Malic acid\")\\\n",
    "    .withColumnRenamed(\"Nonflavanoid.phenols\", \"Nonflavanoid phenols\")\\\n",
    "    .withColumnRenamed(\"Color.int\", \"Color int\")\n",
    "winedf2.show(10)\n",
    "print(winedf2.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.B\n",
    "**TODO:** \n",
    "Discretize all features and apply VectorAssembler. Next apply VectorIndexer and DecisionTreeClassifier like above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       1.0|    1|[13.05,1.73,2.04,...|\n",
      "|       1.0|    1|[13.05,1.77,2.1,1...|\n",
      "|       1.0|    1|[13.16,2.36,2.67,...|\n",
      "|       2.0|    1|[13.24,3.98,2.29,...|\n",
      "|       1.0|    1|[13.29,1.97,2.68,...|\n",
      "|       1.0|    1|[13.41,3.84,2.12,...|\n",
      "|       1.0|    1|[13.5,1.81,2.61,2...|\n",
      "|       1.0|    1|[13.68,1.83,2.36,...|\n",
      "|       1.0|    1|[13.74,1.67,2.25,...|\n",
      "|       1.0|    1|[13.87,1.9,2.8,19...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Test Error = 0.0862069 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "cols = [i for i in winedf2.columns if i != \"label\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "winedf3 = assembler.transform(winedf2).drop(*cols)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = winedf3.randomSplit([0.7, 0.3])\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(winedf3)\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
    "\n",
    "# model = dt.fit(trainingData)\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(10)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.A\n",
    "**TODO:** \n",
    "Build a pipeline consisting of Tokenizer, HashingTF and LogisticRegression, fit it to training data: \n",
    "\n",
    "    (0, \"This example follows simple text document\", 1.0),\n",
    "    (1, \"Ten przykład jest zgodny z prostym dokumentem tekstowym\", 0.0),\n",
    "    (2, \"This example covers concepts of Estimator\", 1.0),\n",
    "    (3, \"Ten przykład obejmuje koncepcje Estymatora\", 0.0)\n",
    "\n",
    "Then test the model on test data:\n",
    "    \n",
    "    (4, \"The image data source is used to load image files from a directory\"),\n",
    "    (5, \"To źródło danych obrazu służy do ładowania plików obrazów z katalogu\"),\n",
    "    (6, \"The LIBSVM data source is used to load libsvm type files from a directory\"),\n",
    "    (7, \"To źródło danych LIBSVM służy do ładowania plików typu libsvm z katalogu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, The image data source is used to load image files from a directory) --> prob=[0.4989747792078863,0.5010252207921136], prediction=1.000000\n",
      "(5, To źródło danych obrazu służy do ładowania plików obrazów z katalogu) --> prob=[0.6713571892386851,0.3286428107613149], prediction=0.000000\n",
      "(6, The LIBSVM data source is used to load libsvm type files from a directory) --> prob=[0.4989747792078863,0.5010252207921136], prediction=1.000000\n",
      "(7, To źródło danych LIBSVM służy do ładowania plików typu libsvm z katalogu) --> prob=[0.6713571892386851,0.3286428107613149], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "\n",
    "trainingData = spark.createDataFrame([\n",
    "    (0, \"This example follows simple text document\", 1.0),\n",
    "    (1, \"Ten przykład jest zgodny z prostym dokumentem tekstowym\", 0.0),\n",
    "    (2, \"This example covers concepts of Estimator\", 1.0),\n",
    "    (3, \"Ten przykład obejmuje koncepcje Estymatora\", 0.0)], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "\n",
    "testData = spark.createDataFrame([\n",
    "    (4, \"The image data source is used to load image files from a directory\"),\n",
    "    (5, \"To źródło danych obrazu służy do ładowania plików obrazów z katalogu\"),\n",
    "    (6, \"The LIBSVM data source is used to load libsvm type files from a directory\"),\n",
    "    (7, \"To źródło danych LIBSVM służy do ładowania plików typu libsvm z katalogu\")], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "\n",
    "prediction = model.transform(testData)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
